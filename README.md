# llm-leakage

LLMs have transformed natural language processing, achieving remarkable performance across a variety of tasks, including question answering, summarization, translation, and arithmetic reasoning. A cornerstone of LLM evaluation is the use of standardized benchmarks, which serve as diagnostic tools for measuring reasoning ability and tracking progress over time. 

One popular benchmark is GSM8K \citep{GSM8K}, which measures reasoning and arithmetic skills on grade-school level math word problems. However, since the benchmark is publicly available on HuggingFace, it is likely to appear in model training data, compromising the validity of evaluations. Benchmark leakage leads to inflated performance and inaccurate assessments of generalization. Detecting and addressing leakage is critical for robust evaluation and fair comparison of models.

Leakage can be subtle since even partial exposure to benchmark examples during pretraining can drastically boost performance. Previous work \citep{elangovan-etal-2021-memorization} has shown that benchmarks such as BigBench and HELM have leaked into LLM data. In fact, there have been recent controversies involving Llama4 being potentially trained on certain benchmarks. Identifying leakage requires careful analysis beyond exact string matches. Prior work has focused on English-language datasets and simple n-gram or cosine similarity measures. These approaches fail to generalize and may miss semantic overlap or training-induced memorization \footnote{Consider the sentences ``The cat sat on the mat" and ``A feline was situated on the rug." These sentences have low n-gram overlap but similar meanings. If a model has seen the first sentence in its training data, it might still assign a low perplexity to the second sentence (due to semantic understanding), and the zlib ratio might be high, indicating memorization.}.

We propose a new dataset, GSM1K-French, translated from GSM8K, and evaluate model behavior under controlled exposure. We use fine-tuning and zlib-perplexity to quantify memorization. Our experiments reveal that models likely memorize English benchmarks and generalize better on the French dataset, and we demonstrate that leakage stymies training development due to overfitting.
