{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e21d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-23 22:29:46 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064ba171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.747 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # Can select any from the below:\n",
    "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "    # And also all Instruct versions and Math. Coding verisons!\n",
    "    model_name = \"unsloth/Qwen2.5-3B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3449047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1eebd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Based on given instruction and context, generate an appropriate response\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    responses = examples[\"response\"]\n",
    "    texts = []\n",
    "\n",
    "    for i,j,k  in zip(instructions, contexts,responses):\n",
    "        text = prompt.format(i,j,k) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68794ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on given instruction and context, generate an appropriate response\n",
      "\n",
      "### Instruction:\n",
      "State the Gettysburg Address\n",
      "\n",
      "### Context:\n",
      " \n",
      "\n",
      "### Response:\n",
      " \n",
      "The Gettysburg Address is a speech given by President Abraham Lincoln on November 19, 1åšŽ16, at the dedication of the Soldiers' National Cemetery in Gettysburg, Pennsylvania. The speech was delivered during the American Civil War and is considered one of the greatest speeches in American history. The Gettysburg Address is a short, powerful speech that emphasizes the importance of preserving the Union and the ideals of liberty and equality for all Americans. The speech is often remembered for its memorable opening line, \"Four score and seven years ago,\" which refers to the year 1863, the year the speech was given. The Gettysburg Address is a powerful reminder of the importance of preserving the values of the American Revolution and the ideals of liberty and equality for all Americans.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"State the Gettysburg Address\",  # instruction\n",
    "            \" \",  # context\n",
    "            \" \",  # response\n",
    "        )\n",
    "    ] * 1,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "output = model.generate(**inputs, streamer=text_streamer, max_new_tokens=1028)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22b2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib_ng import zlib_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, tokenizer, input):\n",
    "    tokenized_input = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input, labels=tokenized_input[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "def zlib_entropy(input):\n",
    "    text_bytes = input.encode('utf-8')\n",
    "    compressed_data = zlib_ng.compress(text_bytes)\n",
    "    entropy_bits = len(compressed_data) * 8\n",
    "    return entropy_bits\n",
    "\n",
    "def zlib_perplexity_ratio(model, tokenizer, input):\n",
    "    perplexity_value = perplexity(model, tokenizer, input)\n",
    "    entropy_value = zlib_entropy(input)\n",
    "    return entropy_value, perplexity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c381116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040, 1.6970635652542114)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zlib_perplexity_ratio(model, tokenizer, \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
